{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29ad263",
   "metadata": {},
   "source": [
    "# Exploration on \n",
    "1) Sentiment analysis of financial news\n",
    "2) Analysts' rating\n",
    "3) LLM to interpret technical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71df1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import finnhub\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94431c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covers 9 major GICS sectors (IT, Communication Services, Consumer Discretionary, Financials, Health Care, Energy, Consumer Staples, Industrials, Utilities, Materials).\n",
    "# Includes mega-cap leaders (AAPL, MSFT, AMZN) and sector representatives (DUK for utilities, NEM for materials).\n",
    "# Balanced enough for cross-sectional hypothesis tests on sentiment vs. returns.\n",
    "\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'META', 'AMZN', 'TSLA', 'JPM', 'MS', 'UNH', 'PFE', \\\n",
    "    'XOM', 'CVX', 'PG', 'KO', 'CAT', 'UNP', 'NEE', 'DUK', 'LIN', 'NEM', \\\n",
    "    'NVDA', 'CSCO', 'NFLX', 'DIS', 'HD', 'MCD', 'BAC', 'WFC', 'JNJ', 'LLY', \\\n",
    "    'COP', 'SLB', 'PEP', 'WMT', 'GE', 'MMM', 'SO', 'AEP', 'SHW', 'FCX',\\\n",
    "    'AVGO', 'INTC', 'CHTR', 'TMUS', 'NKE', 'SBUX', 'AXP', 'BLK', 'MRK', 'ABBV',\\\n",
    "    'MPC', 'PSX', 'KMB', 'K', 'HON', 'LMT', 'D', 'EXC', 'DD', 'ALB',\\\n",
    "    'QCOM', 'ORCL', 'WBD', 'EA', 'LOW', 'BKNG', 'C', 'SCHW', 'BMY', 'GILD', \\\n",
    "    'EOG', 'HAL', 'KHC', 'ADM', 'RTX', 'DE', 'ETR', 'XEL', 'NUE', 'PPG',\\\n",
    "    'AMAT', 'ADBE', 'ROKU', 'TTWO', 'TGT', 'MAR', 'PRU', 'GS', 'AMGN', 'MRNA', \\\n",
    "    'COP', 'OXY', 'CL', 'TSN', 'FDX', 'ITW', 'SRE', 'PPL', 'STLD', 'MOS',\\\n",
    "    'USB', 'PYPL', 'HUM', 'CME', 'REGN', 'VLO', 'MDLZ', 'CAT', 'NOW', 'INTU', \\\n",
    "    'VZ', 'KR', 'CMG', 'CI', 'PH', 'BKR', 'NOC', 'IP', 'MLM', 'SNAP',\\\n",
    "    'ADI', 'KLAC', 'D', 'LYV', 'TJX', 'BBWI', 'PNC', 'ICE', 'DXCM', 'EW', \\\n",
    "    'APA', 'DVN', 'EL', 'BG', 'CMI', 'ROK', 'NI', 'ATO', 'AVY', 'BALL',\\\n",
    "    'MU', 'T', 'PINS', 'DLTR', 'EXPE', 'BAX', 'JCI', 'DOV', 'PEG', 'PKG', 'CF',\\\n",
    "    'STX', 'CDNS', 'WMG', 'LVS', 'ALL', 'AON', 'ZBH', 'EMN', 'ISRG', 'FOXA', 'YUM' \\\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f109cd",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sigrid/Desktop/DSML/projects/final_project/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a model from HuggingFace\n",
    "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f2ec672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_percentile(headline):\n",
    "    # Run sentiment analysis\n",
    "    # for headline in news_headlines:\n",
    "    inputs = tokenizer(headline, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    # probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "\n",
    "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "    pred_idx = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    pred_label = labels[pred_idx]\n",
    "    confidence = probs[pred_idx]\n",
    "\n",
    "    # Map to percentile scale\n",
    "    if pred_label == \"negative\":\n",
    "        percentile = (1-confidence) * 0.3333\n",
    "    elif pred_label == \"neutral\":\n",
    "        if probs[0] > probs[2]:  # leaning negative\n",
    "            percentile = 0.3334 + confidence/2 * (0.5 - 0.3334)\n",
    "        else:  ## leaning positive\n",
    "            percentile = 0.5 + confidence/2 * (0.6667 - 0.5)\n",
    "    else:  # positive\n",
    "        percentile = 0.6667 + confidence * (1.0 - 0.6667)\n",
    "\n",
    "    return {\n",
    "        \"headline\": headline,\n",
    "        \"sentiment\": pred_label,\n",
    "        \"confidence\": round(confidence, 4),\n",
    "        \"percentile\": round(percentile, 4)\n",
    "    }\n",
    "\n",
    "with open(\"seekingalpha_session_id.pkl\", \"rb\") as handle:\n",
    "    sa_session_id = pickle.load(handle)\n",
    "    \n",
    "with open(\"seekingalpha_api_key.pkl\", \"rb\") as handle:\n",
    "    sa_api_key = pickle.load(handle)\n",
    "\n",
    "# Replace with your RapidAPI key\n",
    "API_KEY = sa_api_key\n",
    "BASE_HOST = \"seeking-alpha.p.rapidapi.com\"\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Key\": API_KEY,\n",
    "    \"X-RapidAPI-Host\": BASE_HOST\n",
    "}\n",
    "\n",
    "def fetch_news_list(symbol, page=1, size=5):\n",
    "    url = f\"https://{BASE_HOST}/news/v2/list-by-symbol\"\n",
    "    params = {\"id\": symbol, \"page\": str(page), \"size\": str(size)}\n",
    "    r = requests.get(url, headers=headers, params=params)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_detail(item):\n",
    "    item_id = item[\"id\"]\n",
    "    item_type = item.get(\"type\")\n",
    "\n",
    "    if item_type == \"news\":\n",
    "        url = f\"https://{BASE_HOST}/news/v2/get-details\"\n",
    "    elif item_type == \"article\":\n",
    "        url = f\"https://{BASE_HOST}/articles/v2/get-details\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    params = {\"id\": str(item_id)}\n",
    "    r = requests.get(url, headers=headers, params=params)\n",
    "    if r.status_code == 404:\n",
    "        return None\n",
    "    r.raise_for_status()\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33a5ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_time(x, interval):\n",
    "    if (x.minute//interval + 1)*interval == 60:\n",
    "        return x.replace(hour = x.hour+1, minute = 0, second=0)\n",
    "    else:\n",
    "        return x.replace(minute = (x.minute//interval + 1)*interval, second = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d444100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns today's date in the format year-month-day\n",
    "    \"\"\"\n",
    "    \n",
    "    return datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "def get_stock_data(stock_name, period='2y', interval='1h'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses the yahoo finance API to extract historical stock price data for a given stock\n",
    "    and a given starting date\n",
    "    \"\"\"\n",
    "    \n",
    "    data = yf.Ticker(stock_name).history(period=period, interval=interval)\n",
    "    data.reset_index(inplace=True)\n",
    "    data[\"Datetime\"] = data[\"Datetime\"].apply(lambda x: str(x)[:-6])\n",
    "    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"])\n",
    "    data['timestamp'] = data['Datetime'].dt.to_pydatetime()\n",
    "    data['target'] = data['Close'].pct_change()*10000\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def get_stock_data_start_end(stock_name, start, end, interval='1h'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses the yahoo finance API to extract historical stock price data for a given stock\n",
    "    and a given starting date\n",
    "    \"\"\"\n",
    "    \n",
    "    data = yf.Ticker(stock_name).history(start = start, end = end, interval=interval)\n",
    "    data.reset_index(inplace=True)\n",
    "    data[\"Datetime\"] = data[\"Datetime\"].apply(lambda x: str(x)[:-6])\n",
    "    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"])\n",
    "    data['timestamp'] = data['Datetime'].dt.to_pydatetime()\n",
    "    data['target'] = data['Close'].pct_change()*10000\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d7f23aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def get_sentiment_with_return(ticker, interval):\n",
    "    resp = fetch_news_list(ticker, page=10, size=200)\n",
    "    news_dict = {}\n",
    "\n",
    "    for item in resp.get(\"data\", []):\n",
    "        title = item[\"attributes\"].get(\"title\")\n",
    "        dt = item[\"attributes\"].get(\"publishOn\")\n",
    "        sent_score = sentiment_percentile(title)['percentile']\n",
    "        news_dict[dt] = [title, sent_score]\n",
    "    news_df = pd.DataFrame(news_dict).transpose()\n",
    "    news_df.columns = ['news', 'score']\n",
    "\n",
    "    news_df.insert(0, 'Datetime', pd.to_datetime(news_df.index.str[:19]))\n",
    "    news_df['Datetime'] = news_df['Datetime'].apply(lambda x: round_up_time(x, interval))\n",
    "    news_df.sort_values('Datetime', inplace=True)\n",
    "\n",
    "    if ((news_df['Datetime'].dt.date.max() - news_df['Datetime'].dt.date.min()).days>60):\n",
    "        stock = get_stock_data_start_end(ticker, start = news_df['Datetime'].dt.date.max() + timedelta(days=-25), end = news_df['Datetime'].dt.date.max(), interval=str(interval)+'m')\n",
    "    else:\n",
    "        stock = get_stock_data_start_end(ticker, start = news_df['Datetime'].dt.date.min(), end = news_df['Datetime'].dt.date.max() + timedelta(days=1), interval=str(interval)+'m')\n",
    "\n",
    "    df = news_df[['Datetime', 'news', 'score']].merge(stock[['Datetime', 'Close']], left_on='Datetime', right_on='Datetime', how='left')\n",
    "    df.sort_values('Datetime', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df['pos'] = df['score'].apply(lambda x: -1 if (x<0.3334) else (1 if x > 0.6666 else 0))\n",
    "    df['cumu_ret'] = ((df['Close'].pct_change().shift(-1) * df['pos']).cumsum()+1).cumprod()-1\n",
    "\n",
    "    return news_df, stock, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6eb92",
   "metadata": {},
   "source": [
    "## Hypothesis test: using sentiment score does yield positive return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d22d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta: effect, how far above 0 you want to detect\n",
    "# sigma: variability of your data\n",
    "# power = 1-beta: probability of detecting the effect if it exists (commonly 80%)\n",
    "def required_n_one_sided(alpha=0.05, power=0.8, sigma=0.02, delta=0.0005):\n",
    "    z_alpha = norm.ppf(1 - alpha)      # 1.645 for alpha=0.05\n",
    "    z_beta  = norm.ppf(power)          # 0.842 for power=0.8\n",
    "    return ((z_alpha + z_beta)**2 * sigma**2) / (delta**2)\n",
    "\n",
    "def ticker_years_needed(n_required, p_positive=0.5, years=1, days_per_year=252):\n",
    "    obs_per_ticker_year = p_positive * days_per_year\n",
    "    return n_required / obs_per_ticker_year / years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "72cda4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required observations: 1583\n",
      "Approx tickers for 22 trading days: 158\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "n = required_n_one_sided(alpha=0.05, power=0.8, sigma=0.008, delta=0.0005)\n",
    "tickers_for_1yr = ticker_years_needed(n, p_positive=0.5, years=0.08)\n",
    "print(\"Required observations:\", int(np.ceil(n)))\n",
    "print(\"Approx tickers for 22 trading days:\", int(np.ceil(tickers_for_1yr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15374625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "stock_return = []\n",
    "for tk in tickers:\n",
    "    news_df, stock, df = get_sentiment_with_return(tk, 2)\n",
    "    stock_return.append([tk, df['cumu_ret'].dropna().iloc[-1]])\n",
    "stk_df = pd.DataFrame(stock_return)\n",
    "stk_df.columns = ['Ticker', 'Return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f764898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed mean: 0.17664461679627103\n",
      "Bootstrap p-value (H0: mean <= 0): 0.0063\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap with Hypothesis testing\n",
    "n = len(stk_df['Return'])\n",
    "observed_mean = np.mean(stk_df['Return'])\n",
    "\n",
    "# Bootstrap\n",
    "n_boot = 10000\n",
    "boot_means = []\n",
    "for _ in range(n_boot):\n",
    "    sample = np.random.choice(stk_df['Return'], size=int(n*0.8), replace=True)\n",
    "    boot_means.append(np.mean(sample))\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "\n",
    "# One-sided p-value: probability mean <= 0\n",
    "p_value = np.mean(boot_means <= 0)\n",
    "\n",
    "print(\"Observed mean:\", observed_mean)\n",
    "print(\"Bootstrap p-value (H0: mean <= 0):\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb2d5c5",
   "metadata": {},
   "source": [
    "Since p_value is less than 0.05, we could reject the null hypothesis, i.e. accpet that using sentiment to trade stocks does yield positive returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702427d3",
   "metadata": {},
   "source": [
    "# 2. Analysts' Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6adebf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finnhub_api_key.pkl\", \"rb\") as handle:\n",
    "    finnhub_api_key = pickle.load(handle)\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "\n",
    "def get_recommendation_trends(ticker):\n",
    "    data = finnhub_client.recommendation_trends(ticker)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.rename(columns={\n",
    "        \"period\": \"Date\",\n",
    "        \"strongBuy\": \"Strong Buy\",\n",
    "        \"buy\": \"Buy\",\n",
    "        \"hold\": \"Hold\",\n",
    "        \"sell\": \"Sell\",\n",
    "        \"strongSell\": \"Strong Sell\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    df['Score'] = (df['Strong Buy'] * 2 + df['Buy'] + df['Sell'] * (-1) + df['Strong Sell'] * (-2))/df[['Strong Buy', 'Buy', 'Hold', 'Sell', 'Strong Sell']].sum(axis=1)\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "    \n",
    "    return df[['Date', 'Strong Buy', 'Buy', 'Hold', 'Sell', 'Strong Sell', 'Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afa18f",
   "metadata": {},
   "source": [
    "## Hypothesis test (H0: Excess rerurn is smaller or equal to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe87594",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for tk in tickers:    \n",
    "    rec = get_recommendation_trends(tk)\n",
    "    stock = yf.Ticker(tk).history(start = rec['Date'].min(), end = rec['Date'].max() + timedelta(days = 31), interval='1mo')\n",
    "    stock.index = pd.to_datetime(stock.index)\n",
    "    stock.reset_index(drop=False, inplace=True)\n",
    "    stock['Date'] = stock['Date'].dt.date\n",
    "\n",
    "    df = stock[['Date','Close']].merge(rec[['Date', 'Score']], left_on='Date', right_on='Date', how='left')\n",
    "    df['LongOnly_Ret'] = (df['Close'].pct_change().shift(-1) * abs(df['Score']).mean()+1).cumprod()-1\n",
    "    df['Strat_Ret'] = (df['Close'].pct_change().shift(-1) * round(df['Score'],2)+1).cumprod()-1\n",
    "\n",
    "    exc_ret = (df['Strat_Ret'].dropna().iloc[-1] - df['LongOnly_Ret'].dropna().iloc[-1]+1)**(12/(df.shape[0]-1))-1\n",
    "    ret.append([tk, exc_ret])\n",
    "\n",
    "ret_df = pd.DataFrame(ret)\n",
    "ret_df.columns = ['Ticker', 'ExcessReturn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0642c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed mean: -0.0024268233959812077\n",
      "Bootstrap p-value (H0: mean <= 0): 0.7281\n"
     ]
    }
   ],
   "source": [
    "n = len(ret_df['ExcessReturn'])\n",
    "observed_mean = np.mean(ret_df['ExcessReturn'])\n",
    "\n",
    "# Bootstrap\n",
    "n_boot = 10000\n",
    "boot_means = []\n",
    "for _ in range(n_boot):\n",
    "    sample = np.random.choice(ret_df['ExcessReturn'], size=n, replace=True)\n",
    "    boot_means.append(np.mean(sample))\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "\n",
    "# One-sided p-value: probability mean <= 0\n",
    "p_value = np.mean(boot_means <= 0)\n",
    "\n",
    "print(\"Observed mean:\", observed_mean)\n",
    "print(\"Bootstrap p-value (H0: mean <= 0):\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f17efc",
   "metadata": {},
   "source": [
    "Can't reject the null hypothesis, so will disregard this trading signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a51dc",
   "metadata": {},
   "source": [
    "# 3. LLM on Technical Chart Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41728787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import SMAIndicator\n",
    "from openai import OpenAI    # or your preferred client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce53b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_caption(df):\n",
    "    # compute simple indicators\n",
    "    df = df.copy()\n",
    "    df['sma20'] = SMAIndicator(df['Close'], window=20).sma_indicator()\n",
    "    df['sma50'] = SMAIndicator(df['Close'], window=50).sma_indicator()\n",
    "    df['rsi'] = RSIIndicator(df['Close'], window=14).rsi()\n",
    "\n",
    "    last = df.iloc[-1]\n",
    "    window_start = df['Date'].iloc[0]\n",
    "    window_end = df['Date'].iloc[-1]\n",
    "    ret = (last['Close']/df.iloc[0]['Close'] - 1) * 100\n",
    "\n",
    "    caption = (\n",
    "        f\"Window: {window_start} to {window_end} ({len(df)} bars)\\n\"\n",
    "        f\"Price: open {df.iloc[0]['Open']:.2f}, close {last['Close']:.2f}, high {df['High'].max():.2f}, low {df['Low'].min():.2f}\\n\"\n",
    "        f\"Return: {ret:.2f}% over window\\n\"\n",
    "        f\"Trend: sma20 {last['sma20']:.2f} vs sma50 {last['sma50']:.2f}\\n\"\n",
    "        f\"Momentum: RSI(14)={last['rsi']:.1f}\\n\"\n",
    "    )\n",
    "    # add simple divergence detection (toy)\n",
    "    caption += detect_simple_divergence(df)\n",
    "    return caption\n",
    "\n",
    "def detect_simple_divergence(df):\n",
    "    # naive: compare last high vs previous high and RSI\n",
    "    highs = df['High']\n",
    "    rsi = df['rsi']\n",
    "    idx_highs = highs.nlargest(2).index.tolist()\n",
    "    if len(idx_highs) >= 2:\n",
    "        latest, prev = idx_highs[0], idx_highs[1]\n",
    "        if highs.iloc[latest] > highs.iloc[prev] and rsi.iloc[latest] < rsi.iloc[prev]:\n",
    "            return \"Anomaly: bearish divergence detected between price highs and RSI.\\n\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69283d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekTechnicalAnalyst:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.deepseek.com/v1/chat/completions\"  # Correct API endpoint\n",
    "        self.system_prompt = \"\"\"You are a technical analyst specializing in stock market analysis. \n",
    "        Analyze the provided structured caption data and provide a technical bias assessment.\n",
    "\n",
    "        Respond with ONLY a JSON object in this exact format:\n",
    "        {\n",
    "            \"score\": -0.7,\n",
    "            \"label\": \"Bearish\",\n",
    "            \"reasoning\": \"Brief technical reasoning here\"\n",
    "        }\n",
    "\n",
    "        Scoring scale:\n",
    "        - +1.0 to +0.7: Very Bullish\n",
    "        - +0.7 to +0.3: Bullish  \n",
    "        - +0.3 to -0.3: Neutral\n",
    "        - -0.3 to -0.7: Bearish\n",
    "        - -0.7 to -1.0: Very Bearish\n",
    "\n",
    "        Analyze based on:\n",
    "        - Price position relative to moving averages\n",
    "        - RSI momentum readings\n",
    "        - Trend structure\n",
    "        - Support/resistance levels\"\"\"\n",
    "\n",
    "    def call_deepseek_api(self, user_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"Make API call to DeepSeek LLM\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"deepseek-chat\",  # or \"deepseek-coder\" depending on your needs\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 500,\n",
    "            \"response_format\": {\"type\": \"json_object\"}  # Force JSON response\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.base_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response status: {e.response.status_code}\")\n",
    "                print(f\"Response body: {e.response.text}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_technical_data(self, caption: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis function using DeepSeek API\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "        Analyze this technical data and provide bias assessment:\n",
    "\n",
    "        {caption}\n",
    "        \"\"\"\n",
    "\n",
    "        api_response = self.call_deepseek_api(user_prompt)\n",
    "        \n",
    "        if api_response is None:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"label\": \"API Error\",\n",
    "                \"reasoning\": \"Failed to get analysis from API\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Extract the JSON response from the API\n",
    "            llm_response = api_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            analysis = json.loads(llm_response)\n",
    "            return analysis\n",
    "        except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"label\": \"Parse Error\",\n",
    "                \"reasoning\": f\"Could not parse API response: {e}\"\n",
    "            }\n",
    "        \n",
    "# Alternative: Simple function for quick use\n",
    "def analyze_stock_caption(caption: str) -> Dict[str, Any]:\n",
    "    with open(\"deepseek_api_key.pkl\", \"rb\") as handle:\n",
    "        api_key = pickle.load(handle)\n",
    "    \"\"\"One-line function to analyze stock technical data\"\"\"\n",
    "    analyst = DeepSeekTechnicalAnalyst(api_key)\n",
    "    return analyst.analyze_technical_data(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_ticker = random.choices(tickers, k=3)\n",
    "ta_dfs = []\n",
    "for tk in sel_ticker:\n",
    "    stock = yf.Ticker(tk).history(start = '2025-05-01', end = '2026-01-01', interval='1d')\n",
    "    stock.drop(['Dividends', 'Stock Splits'], axis=1, inplace=True)\n",
    "    stock.reset_index(drop=False, inplace=True)\n",
    "    stock['Date'] = pd.to_datetime(stock['Date']).dt.date\n",
    "    caption = make_caption(stock)\n",
    "\n",
    "    score = []\n",
    "    for i in range(55, len(stock)):\n",
    "        stock_prior = stock.loc[:i-1].copy()\n",
    "        caption = make_caption(stock_prior)\n",
    "        result = analyze_stock_caption(caption)\n",
    "        score.append([stock.loc[i, 'Date'], result['score']])\n",
    "    score_df = pd.DataFrame(score)\n",
    "    score_df.columns = ['Date', 'Score']\n",
    "    score_df['Date'] = pd.to_datetime(score_df['Date']).dt.date\n",
    "\n",
    "    df = score_df.merge(stock[['Date', 'Close']], left_on='Date', right_on='Date', how='left')\n",
    "    df['Ret'] = (df['Close'].pct_change() * df['Score']+1).cumprod()-1\n",
    "    df['LongOnly_Ret'] = (df['Close'].pct_change() * abs(df['Score']).mean() + 1).cumprod()-1\n",
    "\n",
    "    ta_dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8ae6d",
   "metadata": {},
   "source": [
    "## Hypothesis test (H0: the excess return of technical analysis is smaller or equal to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ec73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_df = pd.DataFrame(columns = ['Ret', 'LongOnly_Ret'])\n",
    "for i in range(len(ta_dfs)):\n",
    "    ta_df = pd.concat([ta_df, ta_dfs[i][['Ret', 'LongOnly_Ret']]], axis=0)\n",
    "ta_df['ExcessReturn'] = ta_df['Ret'] - ta_df['LongOnly_Ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35e736d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed mean: -0.035843172515338396\n",
      "Bootstrap p-value (H0: mean <= 0): 0.1603\n"
     ]
    }
   ],
   "source": [
    "n = len(ta_df['ExcessReturn'])\n",
    "observed_mean = np.mean(ta_df['ExcessReturn'])\n",
    "\n",
    "# Bootstrap\n",
    "n_boot = 10000\n",
    "boot_means = []\n",
    "for _ in range(n_boot):\n",
    "    sample = np.random.choice(ta_df['ExcessReturn'], size=int(n*0.6), replace=True)\n",
    "    boot_means.append(np.mean(sample))\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "\n",
    "# One-sided p-value: probability mean <= 0\n",
    "p_value = np.mean(boot_means <= 0)\n",
    "\n",
    "print(\"Observed mean:\", observed_mean)\n",
    "print(\"Bootstrap p-value (H0: mean <= 0):\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102e972",
   "metadata": {},
   "source": [
    "Fail to reject H0, i.e. using LLM for technical does NOT improve the return."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
